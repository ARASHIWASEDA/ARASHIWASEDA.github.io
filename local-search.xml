<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>第四章：NKD算法实现</title>
    <link href="/2023/07/29/%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9ANKD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/07/29/%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9ANKD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>这次介绍的知识蒸馏算法叫做Normalized KnowledgeDistillation，不是一个被单独提出来的知识蒸馏算法，但本人比较感兴趣，所以专门拿出来介绍一下。</p><h2 id="原文简单介绍">原文简单介绍</h2><p>论文链接：<a href="https://arxiv.org/pdf/2303.13005">From KnowledgeDistillation to Self-Knowledge Distillation: A Unified Approach withNormalized Loss and Customized Soft Labels</a>。</p><p>这篇文章实际上一共提出了两个知识蒸馏的模块，一个是NKD，另一个是USKD。感觉论文的重点其实是SelfKD，但是本人单独跑了NKD的部分之后觉得效果很好，也可以直接耦合到其他KD算法中。不过NKD的部分与之前介绍的DKD算法比较相似，作者可能是考虑到这一点没有单独把NKD作为一个创新点发表。</p><p>之前介绍的DKD算法是把目标类和非目标类分别计算KL散度当作蒸馏的损失函数，而NKD也是把目标类和非目标类分别进行蒸馏，但在损失函数的选择和划分的策略上略有不同。</p><p>依然还是先搬出来交叉熵损失函数的公式： <span class="math display">\[L_{ori}=-\sum^C_iV_ilog(S_i)=-log(S_i)\]</span> 如果把交叉熵应用与知识蒸馏，损失函数可以变成这样： <spanclass="math display">\[L_{kd}=-\sum^C_iT_ilog(S_i)=-T_ilog(S_i)-\sum^C_{i\neq{t}}T_ilog(S_i)\]</span> 引入一个Normalize函数<spanclass="math inline">\(N()\)</span>对输出进行处理，将上式变形如下： <spanclass="math display">\[L_{nkd}=-T_tlog(S_t)-\gamma\cdot\lambda^2\cdot\sum^C_{i\neq{t}}N(T^\lambda_i)log(N(S^\lambda_i))\]</span> 这里面的<spanclass="math inline">\(\lambda\)</span>实际是进行temperature计算的意思。到这里NKD的蒸馏损失函数就构造完毕了，看起来很简单，但其实效果很好。深度学习里很多技巧都是简单而有效的，NKD也有一点这个感觉了。</p><p>然后我们对比一下NKD和DKD的区别在哪，DKD的函数如下： <spanclass="math display">\[L_{dkd}=p^T_tlog(\frac{p^T_t}{p^S_t})+p^T_{\backslash{t}}log(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})+p^T_{\backslash{t}}\sum^C_{i=1,i\neq{t}}\hat{p}^T_ilog(\frac{\hat{p}^T_i}{\hat{p}^S_i})\\L_{dkd}=\alpha{TCKD}+\beta{NCKD}\]</span>乍看之下，两者的区别似乎很大，但其实主要的区别是在于DKD引入了一个新的<spanclass="math inline">\(p^T_{\backslash{t}}\)</span>变量，而且两者使用的损失函数不同：DKD用的KL散度，而NKD用的交叉熵。</p><h2 id="nkd损失实现">NKD损失实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">nkd_loss</span>(<span class="hljs-params">logits_student, logits_teacher, label, gamma, temperature=<span class="hljs-number">1.</span></span>):<br>    target = label.reshape(-<span class="hljs-number">1</span>)<br>    target = target.unsqueeze(<span class="hljs-number">1</span>)<br><br>    N, c = logits_student.shape<br>    <span class="hljs-comment"># 函数中的第一部分</span><br>    log_pred_student = F.log_softmax(logits_student, dim=<span class="hljs-number">1</span>)<br>    pred_teacher = F.softmax(logits_teacher, dim=<span class="hljs-number">1</span>)<br><br>    target_student = torch.gather(log_pred_student, <span class="hljs-number">1</span>, label)  <span class="hljs-comment"># gather的作用是把目标类别的值取出来</span><br>    target_teacher = torch.gather(pred_teacher, <span class="hljs-number">1</span>, label)  <span class="hljs-comment"># shape: (batch_size,1)</span><br>    tckd_loss = -(target_student * target_teacher).mean()<br><br>    <span class="hljs-comment"># 函数中的第二部分</span><br>    mask = torch.ones_like(logits_student).scatter_(<span class="hljs-number">1</span>, label, <span class="hljs-number">0</span>).<span class="hljs-built_in">bool</span>()  <span class="hljs-comment"># scatter的作用是把目标类别的mask置0</span><br>    logits_student = logits_student[mask].reshape(N, -<span class="hljs-number">1</span>)<br>    logits_teacher = logits_teacher[mask].reshape(N, -<span class="hljs-number">1</span>)<br><br>    non_target_student = F.log_softmax(logits_student / temperature, dim=<span class="hljs-number">1</span>)<br>    non_target_teacher = F.softmax(logits_teacher / temperature, dim=<span class="hljs-number">1</span>)<br><br>    nckd_loss = -(non_target_student, non_target_teacher).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>).mean()<br>    <span class="hljs-keyword">return</span> tckd_loss + gamma * (temperature ** <span class="hljs-number">2</span>) * nckd_loss<br></code></pre></td></tr></table></figure><h2 id="实现nkd蒸馏器的类">实现NKD蒸馏器的类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@register_distiller  </span><span class="hljs-comment"># 装饰器，注册NKD，具体用法参考本博客第二章</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NKD</span>(<span class="hljs-title class_ inherited__">BaseDistiller</span>):  <span class="hljs-comment"># 蒸馏器继承于BaseDistiller，定义来自于第二章</span><br>    requires_feature = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 指示蒸馏方法是否需要中间层特征</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher, criterion, args</span>):<br>        <span class="hljs-built_in">super</span>(NKD, self).__init__(student, teacher, criterion, args)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, image, labels</span>):<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            self.teacher.<span class="hljs-built_in">eval</span>()<br>            logits_teacher = self.teacher(image)<br>        logits_student = self.student(image)<br>        loss_gt = self.args.gt_loss_weight * self.criterion(logits_student, labels)<br>        loss_kd = self.args.kd_loss_weight * nkd_loss(logits_student, logits_teacher, labels,<br>                                                      self.args.nkd_gamma, self.args.nkd_temperature)<br>        losses_dict = &#123;<br>            <span class="hljs-string">&quot;loss_gt&quot;</span>: loss_gt,<br>            <span class="hljs-string">&quot;loss_kd&quot;</span>: loss_kd,<br>        &#125;<br>        <span class="hljs-keyword">return</span> logits_student, losses_dict<br></code></pre></td></tr></table></figure><p>具体的main函数就不放了，与之前的文章一样，这里简单展示一下用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Distiller=get_distiller(<span class="hljs-string">&quot;name_of_distiller&quot;</span>)  <span class="hljs-comment"># example: get_distiller(&quot;nkd&quot;)</span><br>distiller=Distiller(student,teacher,criterion,args)<br></code></pre></td></tr></table></figure><h2 id="结果展示">结果展示</h2>]]></content>
    
    
    <categories>
      
      <category>知识蒸馏</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
      <tag>timm</tag>
      
      <tag>基于响应的知识蒸馏</tag>
      
      <tag>Normalized knowledge distillation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>实现训练时的时间预测</title>
    <link href="/2023/07/28/%E5%AE%9E%E7%8E%B0%E8%AE%AD%E7%BB%83%E6%97%B6%E7%9A%84%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B/"/>
    <url>/2023/07/28/%E5%AE%9E%E7%8E%B0%E8%AE%AD%E7%BB%83%E6%97%B6%E7%9A%84%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>如果训练的时候想知道大概的结束时间，可以实现一个简单的时间预测类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TimePredictor</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, epochs, history=<span class="hljs-number">30</span></span>):<br>        self.history = history  <span class="hljs-comment"># 保留多少个最近的历史数据</span><br>        self.rest_epochs = epochs  <span class="hljs-comment"># 训练epoch总数</span><br>        self.duration_list = []  <span class="hljs-comment"># 保留最近的epoch训练时间</span><br>        self.prev_time = time.time()  <span class="hljs-comment"># 记录上一次更新时间</span><br>        self.average_duration = <span class="hljs-number">0.</span>  <span class="hljs-comment"># 记录epoch平均训练时间</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self</span>):<br>        temp_time = time.time()<br>        duration = temp_time - self.prev_time<br>        self.prev_time = temp_time<br>        self.duration_list.append(duration)<br>        self.rest_epochs -= <span class="hljs-number">1</span>  <span class="hljs-comment"># 剩余epochs数减1</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(self.duration_list) &gt; self.history):<br>            self.duration_list = self.duration_list[-self.history:]  <span class="hljs-comment"># 仅保留最近的history个记录</span><br>        self.average_duration = np.mean(self.duration_list)  <span class="hljs-comment"># 更新平均训练时间</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_predict</span>(<span class="hljs-params">self</span>):<br>        end_timestamp = self.prev_time + self.average_duration * self.rest_epochs  <span class="hljs-comment"># 计算预测结束时间</span><br>        <span class="hljs-keyword">return</span> datetime.fromtimestamp(end_timestamp).strftime(<span class="hljs-string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>scripts</tag>
      
      <tag>时间预测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第三章：解耦合知识蒸馏算法</title>
    <link href="/2023/07/18/%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E8%A7%A3%E8%80%A6%E5%90%88%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%AE%97%E6%B3%95/"/>
    <url>/2023/07/18/%E7%AC%AC%E4%B8%89%E7%AB%A0%EF%BC%9A%E8%A7%A3%E8%80%A6%E5%90%88%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>这次要实现解耦合知识蒸馏算法。</p><h2 id="原文简单介绍">原文简单介绍</h2><p>DKD算法的全称是Decoupled KnowledgeDistillation，中文就是解耦合知识蒸馏算法，论文链接：<ahref="https://arxiv.org/pdf/2203.08679">2203.08679 (arxiv.org)</a>。</p><p>所谓的解耦合是将原版KD中的蒸馏损失函数进行了重构，划分成了目标类（Targetclass）和非目标类（Non-targetclasses）两个部分，分别对两部分赋予不同的系数可以帮助网络更加关注某一部分。</p><p>一般的softmax函数为 <span class="math display">\[p_i=\frac{exp(z_i)}{\sum^C_{j=1}exp(z_j)}\]</span> 可以根据Target class和Non-target class将其划分为两个部分 <spanclass="math display">\[p_t=\frac{exp(z_t)}{\sum^C_{j=1}exp(z_j)},p_{\backslash{t}}=\frac{\sum^C_{k=1,k\neq{t}}exp(z_k)}{\sum^C_{j=1}exp(z_j)}\]</span> 此外，在所有的非目标类别中，再单独定义一个softmax函数 <spanclass="math display">\[\hat{p_i}=\frac{exp(z_i)}{\sum^C_{j=1,j\neq{t}}exp(z_j)}\]</span> 根据上述公式，可以得到 <span class="math display">\[\hat{p}_i=p_i/p_{\backslash{t}}\]</span>原本的KD算法中，使用KL散度作为蒸馏损失函数，同样也可以将这一部分的损失函数划分为两个部分<span class="math display">\[KD=KL(\mathbf{p}^T||\mathbf{p}^S)=p^T_tlog(\frac{p^T_t}{p^S_t})+\sum^C_{i=1,i\neq{t}}p^T_tlog(\frac{p^T_i}{p^S_i})\]</span> 根据上式进一步重构 <span class="math display">\[\begin{align*}KD&amp;=p^T_tlog(\frac{p^T_t}{p^S_t})+\sum^C_{i=1,i\neq{t}}(\hat{p}^T_ip^T_{\backslash{t}})log(\frac{\hat{p}^T_ip^T_{\backslash{t}}}{\hat{p}^S_ip^S_{\backslash{t}}})\\&amp;=p^T_tlog(\frac{p^T_t}{p^S_t})+p^T_{\backslash{t}}\sum^C_{i=1,i\neq{t}}\hat{p}^T_ilog(\frac{\hat{p}^T_i}{\hat{p}^S_i})+p^T_{\backslash{t}}\sum^C_{i=1,i\neq{t}}\hat{p}^T_ilog(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})\end{align*}\]</span> 因为<spanclass="math inline">\(log(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})\)</span>与<spanclass="math inline">\(i\)</span>无关，所以其实<spanclass="math inline">\(\sum^C_{i=1,i\neq{t}}\hat{p}^T_ilog(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})=log(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})\)</span>，上式变成<span class="math display">\[KD=p^T_tlog(\frac{p^T_t}{p^S_t})+p^T_{\backslash{t}}log(\frac{p^T_{\backslash{t}}}{p^S_{\backslash{t}}})+p^T_{\backslash{t}}\sum^C_{i=1,i\neq{t}}\hat{p}^T_ilog(\frac{\hat{p}^T_i}{\hat{p}^S_i})\]</span>前两项看作目标类的KL损失，后一项看作非目标类的KL损失，则最后的蒸馏损失函数可以变成<span class="math display">\[KD=TCKD+(1-p^T_t)NCKD\]</span> TCKD（Target Class Knowledge Distillation），NCKD（Non-targetClass KnowledgeDistillation）。到这一步，损失计算与原本并无区别。主要的区别在于，作者加入了两个超参数形成最后的损失函数，从而可以调节目标类别和非目标类别对蒸馏的重要程度。<span class="math display">\[KD=\alpha{TCKD}+\beta{NCKD}\]</span></p><h2 id="dkd损失函数实现">DKD损失函数实现</h2><p>在distillation_losses.py文件中添加dkd_loss函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dkd_loss</span>(<span class="hljs-params">logits_student, logits_teacher, label, alpha, beta, kd_temperature</span>):<br>    gt_mask, other_mask = _get_mask(logits_student, label)<br>    <span class="hljs-comment"># 对应公式中的TCKD部分</span><br>    pred_student = F.softmax(logits_student / kd_temperature, dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># temperatur和DKD的损失计算不冲突</span><br>    pred_teacher = F.softmax(logits_teacher / kd_temperature, dim=<span class="hljs-number">1</span>)<br>    pred_student = cat_mask(pred_student, gt_mask, other_mask)<br>    pred_teacher = cat_mask(pred_teacher, gt_mask, other_mask)<br>    log_pred_student = torch.log(pred_student)<br>    tckd_loss = F.kl_div(log_pred_student, pred_teacher, reduction=<span class="hljs-string">&#x27;batchmean&#x27;</span>) * (kd_temperature ** <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># 对应公式中的NCKD部分</span><br>    pred_teacher_second = F.softmax(logits_teacher / kd_temperature - <span class="hljs-number">1000.0</span> * gt_mask, dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># 让target类别的分数趋近0</span><br>    log_pred_student_second = F.log_softmax(logits_student / kd_temperature - <span class="hljs-number">1000.0</span> * gt_mask, dim=<span class="hljs-number">1</span>)<br>    nckd_loss = F.kl_div(log_pred_student_second, pred_teacher_second, reduction=<span class="hljs-string">&#x27;batchmean&#x27;</span>) * (kd_temperature ** <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> alpha * tckd_loss + beta * nckd_loss<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_mask</span>(<span class="hljs-params">logits, label</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    这个函数主要是对目标类别和非目标类别分别生成mask</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    target = label.reshape(-<span class="hljs-number">1</span>)<br>    target = target.unsqueeze(<span class="hljs-number">1</span>)<br>    other_mask = torch.ones_like(logits).scatter_(<span class="hljs-number">1</span>, target, <span class="hljs-number">0</span>).<span class="hljs-built_in">bool</span>()  <span class="hljs-comment"># 1 for non-target classes</span><br>    gt_mask = torch.zeros_like(logits).scatter_(<span class="hljs-number">1</span>, target, <span class="hljs-number">1</span>).<span class="hljs-built_in">bool</span>()  <span class="hljs-comment"># 1 for target classes</span><br>    <span class="hljs-keyword">return</span> gt_mask, other_mask<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cat_mask</span>(<span class="hljs-params">logits, gt_mask, other_mask</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    把TCKD的两部分concatenate到一起</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    target_pred = (logits * gt_mask).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># target_pred:(batch_size,1)</span><br>    other_pred = (logits * other_mask).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>    pred = torch.cat([target_pred, other_pred], dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, 2)</span><br>    <span class="hljs-keyword">return</span> pred<br></code></pre></td></tr></table></figure><h2 id="dkd蒸馏器类实现">DKD蒸馏器类实现</h2><p>这部分和之前的差不多，没什么特殊的，就不多解释了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@register_distiller</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DKD</span>(<span class="hljs-title class_ inherited__">BaseDistiller</span>):<br>    requires_feature = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher, criterion, args</span>):<br>        <span class="hljs-built_in">super</span>(DKD, self).__init__(student, teacher, criterion, args)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, image, label</span>):<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            self.teacher.<span class="hljs-built_in">eval</span>()<br>            logits_teacher = self.teacher(image)<br><br>        logits_student = self.student(image)<br><br>        loss_gt = self.args.gt_loss_weight * self.criterion(logits_student, label)<br>        loss_kd = self.args.kd_loss_weight * dkd_loss(logits_student, logits_teacher, label, self.args.dkd_alpha,<br>                                                      self.args.dkd_beta, self.args.kd_temperature)<br>        losses_dict = &#123;<br>            <span class="hljs-string">&quot;loss_gt&quot;</span>: loss_gt,<br>            <span class="hljs-string">&quot;loss_kd&quot;</span>: loss_kd<br>        &#125;<br>        <span class="hljs-keyword">return</span> logits_student, losses_dict<br></code></pre></td></tr></table></figure><h2 id="程序主体">程序主体</h2><p>和之前有些许不同，更新使用timm.utils.metrics里面的AverageMeter类和accuracy函数去保存和计算指标。此外，在argparser里面加入了dkd-alpha和dkd-beta这两个参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch, distiller, loader, optimizer, args, device, total_iters</span>):<br>    loss_meter = AverageMeter()<br>    top1_meter = AverageMeter()<br>    top5_meter = AverageMeter()<br>    loss_gt_meter = AverageMeter()<br>    loss_kd_meter = AverageMeter()<br>    losses_meter_dict = defaultdict(AverageMeter)<br>    distiller.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        image = images.to(device)<br>        labels = labels.to(device)<br>        logits_student, losses_dict = distiller(image, labels)<br>        loss = <span class="hljs-built_in">sum</span>(losses_dict.values())<br>        top1, top5 = accuracy(logits_student.detach(), labels, topk=(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>))<br>        top1_meter.update(top1.item(), image.size(<span class="hljs-number">0</span>))<br>        top5_meter.update(top5.item(), image.size(<span class="hljs-number">0</span>))<br><br>        loss_meter.update(loss.item(), image.size(<span class="hljs-number">0</span>))<br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> losses_dict:<br>            losses_meter_dict[k].update(losses_dict[k].item(), image.size(<span class="hljs-number">0</span>))<br><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> batch_idx == total_iters - <span class="hljs-number">1</span>:<br>            losses_info = []<br>            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> losses_meter_dict.items():<br>                info = <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;k.capitalize()&#125;</span>: <span class="hljs-subst">&#123;v.avg:<span class="hljs-number">.4</span>f&#125;</span>&#x27;</span><br>                losses_info.append(info)<br>            losses_info = <span class="hljs-string">&#x27;  &#x27;</span>.join(losses_info)<br>            _logger.info(<br>                <span class="hljs-string">f&#x27;Train-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_iters&#125;</span>], &#x27;</span><br>                <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;loss_meter.avg:<span class="hljs-number">.4</span>f&#125;</span> <span class="hljs-subst">&#123;losses_info&#125;</span>, &#x27;</span><br>                <span class="hljs-string">f&#x27;Acc@1: <span class="hljs-subst">&#123;top1_meter.avg:<span class="hljs-number">.2</span>f&#125;</span> Acc@5: <span class="hljs-subst">&#123;top5_meter.avg:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>            )<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval</span>(<span class="hljs-params">epoch, model, loader, args, device, total_iters, loss_func, saver</span>):<br>    loss_meter = AverageMeter()<br>    top1_meter = AverageMeter()<br>    top5_meter = AverageMeter()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">for</span> batch_idx, (image, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>            image = image.to(device)<br>            labels = labels.to(device)<br>            preds = model(image)<br>            loss = loss_func(preds, labels)<br>            top1, top5 = accuracy(preds, labels, topk=(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>))<br>            loss_meter.update(loss.item(), image.size(<span class="hljs-number">0</span>))<br>            top1_meter.update(top1.item(), image.size(<span class="hljs-number">0</span>))<br>            top5_meter.update(top5.item(), image.size(<span class="hljs-number">0</span>))<br>            <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> batch_idx == total_iters - <span class="hljs-number">1</span>:<br>                _logger.info(<br>                    <span class="hljs-string">f&#x27;\tEval-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_iters&#125;</span>], &#x27;</span><br>                    <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;loss_meter.avg:<span class="hljs-number">.4</span>f&#125;</span>, Acc@1: <span class="hljs-subst">&#123;top1_meter.avg:<span class="hljs-number">.2</span>f&#125;</span> Acc@5: <span class="hljs-subst">&#123;top5_meter.avg:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>                )<br>        best_metric, best_epoch = saver.save_checkpoint(epoch, metric=top1_meter.avg)<br>    <span class="hljs-keyword">return</span> best_metric, best_epoch<br><br><br><span class="hljs-comment"># main function</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># process args and some paths</span><br>    args = parser.parse_args()<br>    args_text = yaml.safe_dump(args.__dict__, default_flow_style=<span class="hljs-literal">False</span>)<br>    output_path = args.output <span class="hljs-keyword">if</span> args.output <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;./output/train&#x27;</span><br>    <span class="hljs-keyword">if</span> args.experiment:<br>        exp_name = args.experiment<br>    <span class="hljs-keyword">else</span>:<br>        exp_name = <span class="hljs-string">&#x27;-&#x27;</span>.join([<br>            datetime.now().strftime(<span class="hljs-string">&quot;%Y%m%d-%H%M%S&quot;</span>),<br>            args.model<br>        ])<br>    output_dir = get_outdir(output_path, exp_name)<br>    log_path = os.path.join(output_dir, <span class="hljs-string">&#x27;train.log&#x27;</span>)<br>    checkpoint_dir = os.path.join(output_dir, <span class="hljs-string">&#x27;checkpoints&#x27;</span>)<br>    os.makedirs(checkpoint_dir)<br><br>    <span class="hljs-comment"># setup logging</span><br>    setup_default_logging(log_path=log_path)<br><br>    <span class="hljs-comment"># choose device</span><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>    <span class="hljs-comment"># create dataset</span><br>    <span class="hljs-keyword">if</span> args.dataset == <span class="hljs-string">&#x27;cifar10&#x27;</span>:<br>        transforms = transform.Compose([<br>            transform.Resize(<span class="hljs-number">224</span>),<br>            transform.ToTensor(),<br>            transform.Normalize(mean=(<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), std=(<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>))<br>        ])<br>        train_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms,<br>                                         download=args.dataset_download)<br>        eval_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms,<br>                                        download=args.dataset_download)<br>    <span class="hljs-keyword">else</span>:<br>        transforms = transform.Compose([<br>            transform.Resize(<span class="hljs-number">224</span>),<br>            transform.ToTensor(),<br>            transform.Normalize(mean=((<span class="hljs-number">0.5071</span>, <span class="hljs-number">0.4867</span>, <span class="hljs-number">0.4408</span>)), std=(<span class="hljs-number">0.2675</span>, <span class="hljs-number">0.2565</span>, <span class="hljs-number">0.2761</span>))<br>        ])<br>        train_dataset = datasets.CIFAR100(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms,<br>                                          download=args.dataset_download)<br>        eval_dataset = datasets.CIFAR100(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms,<br>                                         download=args.dataset_download)<br><br>    <span class="hljs-comment"># create dataloader</span><br>    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">True</span>)<br>    eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># create model</span><br>    model = create_model(args.model, pretrained=args.pretrained, num_classes=args.num_classes)<br><br>    <span class="hljs-comment"># create teacher model</span><br>    teacher = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> args.teacher:<br>        teacher = create_model(args.teacher, pretrained=args.teacher_pretrained, num_classes=args.num_classes)<br>        <span class="hljs-keyword">if</span> args.teacher_weight:<br>            load_checkpoint(teacher, args.teacher_weight)<br>        teacher.requires_grad_(<span class="hljs-literal">False</span>)<br>        teacher.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-comment"># create optimizer</span><br>    loss_func = nn.CrossEntropyLoss()<br><br>    <span class="hljs-comment"># create distiller</span><br>    Distiller = get_distiller(args.distiller.lower())<br>    distiller = Distiller(model, teacher, loss_func, args)<br>    distiller.to(device)<br>    optimizer = torch.optim.SGD(distiller.parameters(), lr=args.learning_rate)<br><br>    <span class="hljs-comment"># Train and validation</span><br>    total_train_iters = <span class="hljs-built_in">len</span>(train_loader)<br>    total_eval_iters = <span class="hljs-built_in">len</span>(eval_loader)<br>    best_metric = <span class="hljs-literal">None</span><br><br>    saver = CheckpointSaver(<br>        model=model,<br>        optimizer=optimizer,<br>        args=args,<br>        checkpoint_dir=checkpoint_dir,<br>        recovery_dir=checkpoint_dir,<br>        decreasing=<span class="hljs-literal">False</span>,<br>        max_history=<span class="hljs-number">3</span><br>    )<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(output_dir, <span class="hljs-string">&#x27;args.yaml&#x27;</span>), <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(args_text)<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, args.epochs + <span class="hljs-number">1</span>):<br>            train(epoch=epoch,<br>                  distiller=distiller,<br>                  loader=train_loader,<br>                  optimizer=optimizer,<br>                  args=args,<br>                  device=device,<br>                  total_iters=total_train_iters)<br><br>            best_metric, best_epoch = <span class="hljs-built_in">eval</span>(epoch=epoch,<br>                                           model=model,<br>                                           loader=eval_loader,<br>                                           args=args,<br>                                           device=device,<br>                                           total_iters=total_eval_iters,<br>                                           loss_func=loss_func,<br>                                           saver=saver)<br><br>    <span class="hljs-keyword">except</span> KeyboardInterrupt:<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">if</span> best_metric <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        _logger.info(<span class="hljs-string">f&#x27;***** Best metric is <span class="hljs-subst">&#123;best_metric:<span class="hljs-number">.2</span>f&#125;</span>(<span class="hljs-subst">&#123;best_epoch&#125;</span>) *****.&#x27;</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><h2 id="实验效果">实验效果</h2><p>用resnet18做学生模型，resnet50做教师模型（ACC@153.49），alpha-1，beta-8，temperature-1，最后得到的准确率是50.67。</p><p>之前从零训练resnet18的结果是43.53，所以DKD的效果还是不错的。</p>]]></content>
    
    
    <categories>
      
      <category>知识蒸馏</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
      <tag>timm</tag>
      
      <tag>基于响应的知识蒸馏</tag>
      
      <tag>Decoupled knowledge distillation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第二章：基本的KD实现</title>
    <link href="/2023/07/02/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E6%9C%AC%E7%9A%84KD%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/07/02/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9F%BA%E6%9C%AC%E7%9A%84KD%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>本章节将正式进入知识蒸馏代码的编写部分，目标是简单复现Hinton老爷子提出的最经典的KnowledgeDistillation算法。</p><h2 id="导入各种库">导入各种库</h2><p>首先导入本篇需要用到的各种库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">import</span> yaml<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> datasets<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transform<br><br><span class="hljs-keyword">from</span> timm.utils.log <span class="hljs-keyword">import</span> setup_default_logging<br><span class="hljs-keyword">from</span> timm.utils.summary <span class="hljs-keyword">import</span> get_outdir<br><span class="hljs-keyword">from</span> timm.utils.checkpoint_saver <span class="hljs-keyword">import</span> CheckpointSaver<br><span class="hljs-keyword">from</span> timm.models <span class="hljs-keyword">import</span> create_model, load_checkpoint<br><span class="hljs-keyword">from</span> distillers <span class="hljs-keyword">import</span> get_distiller<br></code></pre></td></tr></table></figure><h2 id="初始化logging">初始化logging</h2><p>设置logging用于输出日志信息，用法可以查看本仓库第一章。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># logger setting</span><br>_logger = logging.getLogger(<span class="hljs-string">&quot;train&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="配置参数">配置参数</h2><p>相较于第一章的参数增加了蒸馏相关配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># configs setting</span><br>parser = argparse.ArgumentParser()<br>parser.add_argument(<span class="hljs-string">&quot;--model&quot;</span>, default=<span class="hljs-string">&quot;resnet18&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;model name&#x27;</span>) <span class="hljs-comment"># 学生模型</span><br>parser.add_argument(<span class="hljs-string">&quot;--learning-rate&quot;</span>, default=<span class="hljs-number">0.01</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;learning rate&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--epochs&quot;</span>, default=<span class="hljs-number">3</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;epochs to train&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--device&quot;</span>, default=<span class="hljs-string">&quot;cuda&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;training device&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--batch-size&quot;</span>, default=<span class="hljs-number">32</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;batch size&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--log-interval&quot;</span>, default=<span class="hljs-number">500</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;to print log info every designated iters&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--output&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;output path to save results&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--experiment&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;name of subfolder of outpur dir&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--num-classes&quot;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;number of classes of the dataset&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--pretrained&quot;</span>, default=<span class="hljs-literal">False</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--dataset-download&quot;</span>, default=<span class="hljs-literal">False</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--dataset&quot;</span>, default=<span class="hljs-string">&#x27;cifar10&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;support cifar10 or cifar100 now&#x27;</span>)<br><br><span class="hljs-comment"># distillation setting</span><br>parser.add_argument(<span class="hljs-string">&quot;--distiller&quot;</span>, default=<span class="hljs-string">&#x27;vanilla&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>) <span class="hljs-comment"># 蒸馏器名字</span><br>parser.add_argument(<span class="hljs-string">&quot;--teacher&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;name of teacher model&#x27;</span>) <span class="hljs-comment"># 教师模型</span><br>parser.add_argument(<span class="hljs-string">&quot;--teacher-pretrained&quot;</span>, default=<span class="hljs-literal">False</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>) <span class="hljs-comment"># 这里激活的话会从timm搭导入在imagenet2012与训练的模型，一般用不到，除非要在imagenet2012上训练学生模型</span><br>parser.add_argument(<span class="hljs-string">&quot;--teacher-weight&quot;</span>, default=<span class="hljs-string">&#x27;./weights/resnet/resnet50_cifar10.pth.tar&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;path of pre-trained teacher weight&#x27;</span>) <span class="hljs-comment"># 教师模型路径</span><br>parser.add_argument(<span class="hljs-string">&quot;--kd-temperature&quot;</span>, default=<span class="hljs-number">4.</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;distillation temperature&#x27;</span>) <span class="hljs-comment"># 知识蒸馏温度</span><br>parser.add_argument(<span class="hljs-string">&quot;--kd-loss-weight&quot;</span>, default=<span class="hljs-number">1.</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>) <span class="hljs-comment"># 知识蒸馏损失系数</span><br>parser.add_argument(<span class="hljs-string">&quot;--gt-loss-weight&quot;</span>, default=<span class="hljs-number">1.</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>) <span class="hljs-comment"># 学生损失函数系数</span><br></code></pre></td></tr></table></figure><h2 id="数据集准备">数据集准备</h2><p>CIFAR10或者CIAFR100。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.dataset == <span class="hljs-string">&#x27;cifar10&#x27;</span>:<br>    transforms = transform.Compose([<br>        transform.Resize(<span class="hljs-number">224</span>),<br>        transform.ToTensor(),<br>        transform.Normalize(mean=(<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), std=(<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>))<br>    ])<br>    train_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms,<br>                                     download=args.dataset_download)<br>    eval_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms,<br>                                    download=args.dataset_download)<br><span class="hljs-keyword">else</span>:<br>    transforms = transform.Compose([<br>        transform.Resize(<span class="hljs-number">224</span>),<br>        transform.ToTensor(),<br>        transform.Normalize(mean=((<span class="hljs-number">0.5071</span>, <span class="hljs-number">0.4867</span>, <span class="hljs-number">0.4408</span>)), std=(<span class="hljs-number">0.2675</span>, <span class="hljs-number">0.2565</span>, <span class="hljs-number">0.2761</span>))<br>    ])<br>    train_dataset = datasets.CIFAR100(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms,<br>                                      download=args.dataset_download)<br>    eval_dataset = datasets.CIFAR100(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms,<br>                                     download=args.dataset_download)<br>    <span class="hljs-comment"># create dataloader</span><br>    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">True</span>)<br>    eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h2 id="模型准备">模型准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create model</span><br>model = create_model(args.model, pretrained=args.pretrained, num_classes=args.num_classes)<br><br><span class="hljs-comment"># create teacher model</span><br>teacher = <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> args.teacher:<br>    teacher = create_model(args.teacher, pretrained=args.teacher_pretrained, num_classes=args.num_classes)<br>    <span class="hljs-keyword">if</span> args.teacher_weight:<br>        load_checkpoint(teacher, args.teacher_weight)<br>    teacher.requires_grad_(<span class="hljs-literal">False</span>)<br>    teacher.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># create optimizer</span><br>loss_func = nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h2 id="蒸馏器准备">蒸馏器准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create distiller</span><br>Distiller = get_distiller(args.distiller.lower())<br>distiller = Distiller(model, teacher, loss_func, args)<br>distiller.to(device)<br>optimizer = torch.optim.SGD(distiller.parameters(), lr=args.learning_rate)<br></code></pre></td></tr></table></figure><p>这段代码中使用的get_distiller和Distiller来自于自定义的函数和类。</p><p>具体的做法是，在train.py下新建文件夹distillers，在distillers文件夹中新建registry.py文件并添加以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br>_distiller_dict = defaultdict()<br><br><span class="hljs-comment"># 这个函数会被用作装饰器，将定义好的蒸馏器类加入到_distiller_dict中，方便后续获取</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">register_distiller</span>(<span class="hljs-params">distiller</span>):<br>    distiller_name = distiller.__name__.lower()<br>    _distiller_dict[distiller_name] = distiller<br>    <span class="hljs-keyword">return</span> distiller<br><br><span class="hljs-comment"># 通过名字获取对应蒸馏器的类</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_distiller</span>(<span class="hljs-params">distiller_name</span>):<br>    <span class="hljs-keyword">return</span> _distiller_dict[distiller_name]<br></code></pre></td></tr></table></figure><p>继续在distillers文件夹中添加文件distillation_losses.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这是基本kd算法中损失函数的实现，后续在这个文件中还会补充其他的损失函数</span><br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">kd_loss</span>(<span class="hljs-params">logits_student, logits_teacher, kd_temperature</span>):<br>    log_pred_student = F.log_softmax(logits_student / kd_temperature, dim=<span class="hljs-number">1</span>)<br>    pred_teacher = F.softmax(logits_teacher / kd_temperature, dim=<span class="hljs-number">1</span>)<br>    loss_kd = F.kl_div(log_pred_student, pred_teacher, reduction=<span class="hljs-string">&#x27;batchmean&#x27;</span>)<br>    loss_kd *= kd_temperature ** <span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> loss_kd<br></code></pre></td></tr></table></figure><p>继续在distillers文件夹中添加文件_base.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现一个基础的蒸馏器类，后续所有的蒸馏器都继承于此类</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseDistiller</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher, criterion, args</span>):<br>        <span class="hljs-built_in">super</span>(BaseDistiller, self).__init__()<br>        self.student = student<br>        self.teacher = teacher<br>        self.criterion = criterion<br>        self.args = args<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        这个函数用来统计参数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        student_params = <span class="hljs-number">0</span><br>        teacher_params = <span class="hljs-number">0</span><br>        extra_params = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> self.named_parameters():<br>            <span class="hljs-keyword">if</span> n.startswith(<span class="hljs-string">&#x27;student&#x27;</span>):<br>                student_params += p.numel()<br>            <span class="hljs-keyword">elif</span> n.startswith(<span class="hljs-string">&#x27;teacher&#x27;</span>):<br>                teacher_params += p.numel()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> p.requires_grad:<br>                    extra_params += p.numel()<br>        <span class="hljs-keyword">return</span> student_params, teacher_params, extra_params<br></code></pre></td></tr></table></figure><p>继续在distillers文件夹中添加文件kd.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现KD类</span><br><span class="hljs-keyword">from</span> .distillation_losses <span class="hljs-keyword">import</span> kd_loss<br><span class="hljs-keyword">from</span> ._base <span class="hljs-keyword">import</span> BaseDistiller<br><span class="hljs-keyword">from</span> .registry <span class="hljs-keyword">import</span> register_distiller<br><span class="hljs-keyword">import</span> torch<br><br><br><span class="hljs-meta">@register_distiller </span><span class="hljs-comment"># 用装饰器将这个类记录下来</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">KD</span>(<span class="hljs-title class_ inherited__">BaseDistiller</span>):<br>    requires_feature = <span class="hljs-literal">False</span> <span class="hljs-comment"># 这个参数指示不需要中间层特征</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher, criterion, args</span>):<br>        <span class="hljs-built_in">super</span>(KD, self).__init__(student, teacher, criterion, args)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, image, label</span>):<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            self.teacher.<span class="hljs-built_in">eval</span>()<br>            logits_teacher = self.teacher(image)<br>        logits_student = self.student(image)<br>        loss_kd = self.args.kd_loss_weight * kd_loss(logits_student, logits_teacher, self.args.kd_temperature)<br>        loss_gt = self.args.gt_loss_weight * self.criterion(logits_student, label)<br>        losses_dict = &#123;<br>            <span class="hljs-string">&#x27;loss_kd&#x27;</span>: loss_kd,<br>            <span class="hljs-string">&#x27;loss_gt&#x27;</span>: loss_gt<br>        &#125;<br>        <span class="hljs-keyword">return</span> logits_student, losses_dict<br></code></pre></td></tr></table></figure><p>继续在distillers文件夹中添加文件vanilla.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 为了让train.py代码结构统一起来，把非蒸馏的普通训练也写成一个类</span><br><span class="hljs-keyword">from</span> ._base <span class="hljs-keyword">import</span> BaseDistiller<br><span class="hljs-keyword">from</span> .registry <span class="hljs-keyword">import</span> register_distiller<br><br><br><span class="hljs-meta">@register_distiller</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vanilla</span>(<span class="hljs-title class_ inherited__">BaseDistiller</span>):<br>    requires_feature = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, student, teacher, criterion, args</span>):<br>        <span class="hljs-built_in">super</span>(Vanilla, self).__init__(student, teacher, criterion, args)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, image, label</span>):<br>        logits_student = self.student(image)<br>        loss_gt = self.criterion(logits_student, label)<br>        losses_dict = &#123;<br>            <span class="hljs-string">&#x27;loss_gt&#x27;</span>: loss_gt<br>        &#125;<br>        <span class="hljs-keyword">return</span> logits_student, losses_dict<br></code></pre></td></tr></table></figure><p>最后在distillers文件夹中添加文件__init__.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 如果后面定义类新的类，这里需要再添加条目</span><br><span class="hljs-keyword">from</span> .vanilla <span class="hljs-keyword">import</span> Vanilla<br><span class="hljs-keyword">from</span> .kd <span class="hljs-keyword">import</span> KD<br><span class="hljs-keyword">from</span> .distillation_losses <span class="hljs-keyword">import</span> kd_loss<br><span class="hljs-keyword">from</span> .registry <span class="hljs-keyword">import</span> get_distiller<br></code></pre></td></tr></table></figure><h2 id="主函数的剩余部分">主函数的剩余部分</h2><p>一部分是路径、logger、device等的设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># process args and some paths</span><br>args = parser.parse_args()<br>args_text = yaml.safe_dump(args.__dict__, default_flow_style=<span class="hljs-literal">False</span>)<br>output_path = args.output <span class="hljs-keyword">if</span> args.output <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;./output/train&#x27;</span><br><span class="hljs-keyword">if</span> args.experiment:<br>    exp_name = args.experiment<br><span class="hljs-keyword">else</span>:<br>    exp_name = <span class="hljs-string">&#x27;-&#x27;</span>.join([<br>        datetime.now().strftime(<span class="hljs-string">&quot;%Y%m%d-%H%M%S&quot;</span>),<br>        args.model<br>    ])<br>output_dir = get_outdir(output_path, exp_name)<br>log_path = os.path.join(output_dir, <span class="hljs-string">&#x27;train.log&#x27;</span>)<br>checkpoint_dir = os.path.join(output_dir, <span class="hljs-string">&#x27;checkpoints&#x27;</span>)<br>os.makedirs(checkpoint_dir)<br><br><span class="hljs-comment"># setup logging</span><br>setup_default_logging(log_path=log_path)<br><br><span class="hljs-comment"># choose device</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><p>一部分是训练和验证的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Train and validation</span><br>total_train_iters = <span class="hljs-built_in">len</span>(train_loader)<br>total_eval_iters = <span class="hljs-built_in">len</span>(eval_loader)<br>best_metric = <span class="hljs-literal">None</span><br><br>saver = CheckpointSaver(<br>    model=model,<br>    optimizer=optimizer,<br>    args=args,<br>    checkpoint_dir=checkpoint_dir,<br>    recovery_dir=checkpoint_dir,<br>    decreasing=<span class="hljs-literal">False</span>,<br>    max_history=<span class="hljs-number">3</span><br>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(output_dir, <span class="hljs-string">&#x27;args.yaml&#x27;</span>), <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(args_text)<br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, args.epochs + <span class="hljs-number">1</span>):<br>        train(epoch=epoch,<br>              distiller=distiller,<br>              loader=train_loader,<br>              optimizer=optimizer,<br>              args=args,<br>              device=device,<br>              total_iters=total_train_iters)<br><br>        best_metric, best_epoch = <span class="hljs-built_in">eval</span>(epoch=epoch,<br>                                       model=model,<br>                                       loader=eval_loader,<br>                                       args=args,<br>                                       device=device,<br>                                       total_iters=total_eval_iters,<br>                                       loss_func=loss_func,<br>                                       saver=saver)<br><br><span class="hljs-keyword">except</span> KeyboardInterrupt:<br>    <span class="hljs-keyword">pass</span><br><span class="hljs-keyword">if</span> best_metric <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    _logger.info(<span class="hljs-string">f&#x27;***** Best metric is <span class="hljs-subst">&#123;best_metric:<span class="hljs-number">.2</span>f&#125;</span>(<span class="hljs-subst">&#123;best_epoch&#125;</span>) *****.&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="训练和验证函数">训练和验证函数</h2><p>为了让主函数看起来更加清晰和美观一点，把训练和验证的代码单独写一个新的函数。</p><h3 id="训练函数">训练函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">epoch, distiller, loader, optimizer, args, device, total_iters</span>):<br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span><br>    distiller.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        image = images.to(device)<br>        labels = labels.to(device)<br>        logits_student, losses_dict = distiller(image, labels)<br>        loss = <span class="hljs-built_in">sum</span>(losses_dict.values())<br>        preds = torch.argmax(logits_student.detach(), dim=<span class="hljs-number">1</span>)<br>        correct += (preds == labels).<span class="hljs-built_in">sum</span>()<br>        total += preds.shape[<span class="hljs-number">0</span>]<br>        acc = <span class="hljs-number">100.0</span> * correct / total<br>        <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> batch_idx == total_iters - <span class="hljs-number">1</span>:<br>            losses_infos = []<br>            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> losses_dict.items():<br>                info = <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;k.capitalize()&#125;</span> - <span class="hljs-subst">&#123;v.item():<span class="hljs-number">.4</span>f&#125;</span>&#x27;</span><br>                losses_infos.append(info)<br>            losses_info = <span class="hljs-string">&#x27;  &#x27;</span>.join(losses_infos)<br>            _logger.info(<br>                <span class="hljs-string">f&#x27;Train-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_iters&#125;</span>], &#x27;</span><br>                <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;losses_info&#125;</span>, Acc: <span class="hljs-subst">&#123;acc:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>            )<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure><h3 id="验证函数">验证函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval</span>(<span class="hljs-params">epoch, model, loader, args, device, total_iters, loss_func, saver</span>):<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        model.<span class="hljs-built_in">eval</span>()<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> batch_idx, (image, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>            image = image.to(device)<br>            labels = labels.to(device)<br>            preds = model(image)<br>            loss = loss_func(preds, labels)<br>            pred_labels = torch.argmax(preds, dim=<span class="hljs-number">1</span>)<br>            correct += (pred_labels == labels).<span class="hljs-built_in">sum</span>()<br>            total += pred_labels.shape[<span class="hljs-number">0</span>]<br>            acc = <span class="hljs-number">100.0</span> * correct / total<br>            <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> batch_idx == total_iters - <span class="hljs-number">1</span>:<br>                _logger.info(<br>                    <span class="hljs-string">f&#x27;\tEval-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_iters&#125;</span>], &#x27;</span><br>                    <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.4</span>f&#125;</span>, Acc: <span class="hljs-subst">&#123;acc:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>                )<br>        best_metric, best_epoch = saver.save_checkpoint(epoch, metric=acc)<br>    <span class="hljs-keyword">return</span> best_metric, best_epoch<br></code></pre></td></tr></table></figure><h2 id="效果展示">效果展示</h2><p>先用vanilla蒸馏器训一个resnet50的模型，在CIFAR10上的准确率是：</p><p>然后用上面得到的resnet50当作教师模型，用resnet18当作学生模型，使用kd蒸馏器，在CIAFR10上训练得到的准确率是：</p><p>这里由于教师模型训练的epoch太少了，性能反而被学生模型超过了。如果多训练几十个epoch的话不会出现这种情况。</p><p>如果不用教师模型，使用vanilla蒸馏器训练resnet18的话，准确率是：</p><p>可以看到kd确实提高了学生模型的性能。</p>]]></content>
    
    
    <categories>
      
      <category>知识蒸馏</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
      <tag>timm</tag>
      
      <tag>基于响应的知识蒸馏</tag>
      
      <tag>Knowledge distillation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一章：实现基础的图像分类模型训练</title>
    <link href="/2023/06/24/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%AE%9E%E7%8E%B0%E5%9F%BA%E7%A1%80%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/"/>
    <url>/2023/06/24/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%AE%9E%E7%8E%B0%E5%9F%BA%E7%A1%80%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<p>知识蒸馏的训练过程与普通的模型训练过程的基本逻辑是相似的，因此在介绍知识蒸馏算法的代码实现之前，让我们先复习一下如何利用timm和torch实现一份基本的图像分类模型训练代码。</p><p>工欲善其事必先利其器，正式编写训练的核心代码前，先介绍一些在许多代码中都有使用的辅助工具。不用这些也没关系，但是随着代码规模的扩大，使用这些库可以帮助提高编写效率。</p><h2 id="argparse">argparse</h2><p>为了在后续的代码中能够方便的管理训练中的诸多参数，推荐使用argparse库。后续如果参数规模增加，可以使用yaml保存部分参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> argparse  <span class="hljs-comment"># 首先引入 argparse库</span><br><br>parser = argparse.ArgumentParser()  <span class="hljs-comment"># 创建一个ArgumentParser类，并添加各种参数</span><br>parser.add_argument(<span class="hljs-string">&quot;--learning-rate&quot;</span>, default=<span class="hljs-number">0.001</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;learning rate&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--epochs&quot;</span>, default=<span class="hljs-number">3</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;epochs to train&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--device&quot;</span>, default=<span class="hljs-string">&quot;cuda&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;training device&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--batch-size&quot;</span>, default=<span class="hljs-number">32</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;batch size&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--log-interval&quot;</span>, default=<span class="hljs-number">500</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;to print log info during every log-interval iterations&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--output&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;output path to save results&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--experiment&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;name of subfolder of outpur dir&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--model&quot;</span>, default=<span class="hljs-string">&quot;resnet34&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;model name&#x27;</span>)<br>parser.add_argument(<span class="hljs-string">&quot;--num-classes&quot;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;number of classes of the dataset&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在代码的主函数部分，可以很方便的获取到设定的参数。还可以将参数形成yaml文件保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> yaml  <span class="hljs-comment"># 用来保存args文件</span><br><br>args = parser.parse_args()<br><span class="hljs-built_in">print</span>(args.model)  <span class="hljs-comment"># &#x27;resnet34&#x27;</span><br>args_text = yaml.safe_dump(args.__dict__, default_flow_style=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(output_dir, <span class="hljs-string">&#x27;args.yaml&#x27;</span>), <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(args_text)<br></code></pre></td></tr></table></figure><h2 id="logging">logging</h2><p>为了能够更加方便的观察训练过程中的各项指标，我们可以使用print函数打印出损失、准确率等参数，也可以使用如tqdm等库实现更加清晰优美的可视化效果。本博客推荐使用logging库实现日志输出和保存的功能，并且timm库提供了一个setup_default_logging函数方便我们设置日志的输出和保存效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging  <span class="hljs-comment"># 首先依然是引入logging库</span><br><span class="hljs-keyword">from</span> timm.utils.log <span class="hljs-keyword">import</span> setup_default_logging  <span class="hljs-comment"># 从timm引入logging的设置函数</span><br><br>_logger = logging.getLogger(<span class="hljs-string">&quot;train&quot;</span>)<br>setup_default_logging(log_path=log_path)  <span class="hljs-comment"># 在指定路径生成日志文件</span><br>_logger.info(<span class="hljs-string">&quot;这是一个logging的示例&quot;</span>)  <span class="hljs-comment"># 这样就会在console输出文字，用法与print相似</span><br></code></pre></td></tr></table></figure><p>点开setup_default_logging函数的源码，看一下是如何实现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FormatterNoInfo</span>(logging.Formatter):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, fmt=<span class="hljs-string">&#x27;%(levelname)s: %(message)s&#x27;</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化一个FormatterNoInfo类，主要目的就是为了格式化日志文件的输出格式</span><br><span class="hljs-string">        如&#x27;%(levelname)s: %(message)s&#x27;，其中levelname代表logging日志的级别</span><br><span class="hljs-string">        有ERROR、INFO、DEBUG等，message就是你要输出的信息内容，具体可以自行搜索</span><br><span class="hljs-string">        _logger.info(&quot;这是一个logging的示例&quot;)的输出为：</span><br><span class="hljs-string">        &#x27;[INFO]: 这是一个logging的示例&#x27;</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        logging.Formatter.__init__(self, fmt)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">format</span>(<span class="hljs-params">self, record</span>):<br>        <span class="hljs-keyword">if</span> record.levelno == logging.INFO:<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">str</span>(record.getMessage())<br>        <span class="hljs-keyword">return</span> logging.Formatter.<span class="hljs-built_in">format</span>(self, record)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_default_logging</span>(<span class="hljs-params">default_level=logging.INFO, log_path=<span class="hljs-string">&#x27;&#x27;</span></span>):<br>    <span class="hljs-comment"># StreamHandler可以将日志信息输出到console</span><br>    console_handler = logging.StreamHandler()<br>    <span class="hljs-comment"># 使用上面定义的类，格式化日志输出</span><br>    console_handler.setFormatter(FormatterNoInfo())<br>    <span class="hljs-comment"># 加入console_handler，不然在console可能没有输出</span><br>    logging.root.addHandler(console_handler)<br>    <span class="hljs-comment"># 设置输出的级别，低于指定级别的信息不会被记录</span><br>    <span class="hljs-comment"># 如果设置级别为INFO，则DEBUG不会被记录，而ERROR、INFO和更高级别的会记录</span><br>    logging.root.setLevel(default_level)<br>    <span class="hljs-comment"># 保存日志文件，如果不设置路径就不保存</span><br>    <span class="hljs-keyword">if</span> log_path:<br>        file_handler = logging.handlers.RotatingFileHandler(log_path, maxBytes=(<span class="hljs-number">1024</span> ** <span class="hljs-number">2</span> * <span class="hljs-number">2</span>), backupCount=<span class="hljs-number">3</span>)<br>        file_formatter = logging.Formatter(<span class="hljs-string">&quot;%(asctime)s - %(name)20s: [%(levelname)8s] - %(message)s&quot;</span>)<br>        file_handler.setFormatter(file_formatter)<br>        logging.root.addHandler(file_handler)<br></code></pre></td></tr></table></figure><p>由于本博客是一个基础的教程，因此基本上只会用到info级别的日志，不用在logging上关注太多，即插即用即可。感兴趣的可以自行搜索学习。</p><h2 id="数据集准备">数据集准备</h2><p>以图像分类举例，可以通过torchvision库获取数据集。出于方便快捷的目的，这里使用CIFAR10数据集，可以自行换成CIFAR100、MNIST、FashionMNIST等等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> datasets<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transform  <span class="hljs-comment"># 读取的数据集需要经过一些变换才可以使用</span><br><br><span class="hljs-comment"># resnet的输入需要（3，224，224）的图片，先不考虑其他的数据增强</span><br><span class="hljs-comment"># 后续会介绍另外一个数据增强库</span><br>transforms = transform.Compose([<br>    transform.Resize(<span class="hljs-number">224</span>),<br>    transform.ToTensor(),<br>    transform.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<br>])<br><span class="hljs-comment"># root是数据集的存储路径，download设置为true可以自动下载，imagenet是下载不了的</span><br>train_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">True</span>, transform=transforms, download=<span class="hljs-literal">True</span>)<br>eval_dataset = datasets.CIFAR10(root=<span class="hljs-string">&quot;datasets&quot;</span>, train=<span class="hljs-literal">False</span>, transform=transforms, download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>此时的数据集还不能在训练中使用，但是我们可以通过索引查看任一数据的内容，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(train_dataset[<span class="hljs-number">0</span>])  <span class="hljs-comment"># 是一个（3，224，224）的tuple</span><br></code></pre></td></tr></table></figure><p>一般使用迭代器的形式遍历数据集中的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader  <span class="hljs-comment"># 首先导入库</span><br><br>train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># shuffle=True的时候，会打乱顺序</span><br>eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h2 id="模型以及相关内容的准备">模型以及相关内容的准备</h2><p>可以使用timm的create_model函数快速的创建需要的模型结构。但是在知识蒸馏中我们可能需要对模型的结构进行修改，这一部分放到后面介绍。先简单的导入一个模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> timm  <span class="hljs-comment"># 导入timm库</span><br><br><span class="hljs-comment"># 下载与训练模型，一般是在imagenet上进行的预训练，具体的训练配置可以到timm.models里面查看</span><br>model = timm.create_model(args.model, pretrained=<span class="hljs-literal">True</span>, num_classes=args.num_classes)  <span class="hljs-comment"># resnet34</span><br>model.cuda()  <span class="hljs-comment"># 将模型转移到GPU设备上</span><br></code></pre></td></tr></table></figure><p>此外，我们至少还需要准备优化器和损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-comment"># optimizer初始化的第一个参数是模型的参数，如果是知识蒸馏代码，这一步会不同</span><br>optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate)<br>loss_func = nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h2 id="模型保存">模型保存</h2><p>训练好的模型需要保存，在正式开始训练之前，我们先设置好模型的保存路径，模型的保存路径取决于以下的两个参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">parser.add_argument(<span class="hljs-string">&quot;--experiment&quot;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>,<br>                    <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;name of subfolder of outpur dir&#x27;</span>)  <span class="hljs-comment"># 这个参数主要是方便我们给每次实验起个名字方便查找</span><br>parser.add_argument(<span class="hljs-string">&quot;--model&quot;</span>, default=<span class="hljs-string">&quot;resnet34&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;model name&#x27;</span>)<br></code></pre></td></tr></table></figure><p>根据提供的参数，我们设置好模型的保存路径：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.experiment:<br>    exp_name = args.experiment<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-comment"># 如果没设置experiment的话就按照时间和模型名字保存</span><br>    exp_name = <span class="hljs-string">&#x27;-&#x27;</span>.join([<br>        datetime.now().strftime(<span class="hljs-string">&quot;%Y%m%d-%H%M%S&quot;</span>),<br>        args.model<br>    ])<br><span class="hljs-comment"># 使用timm快捷创建路径文件夹，可以看作os.makedirs，产生的路径就是</span><br><span class="hljs-comment"># os.path.join(output_path,exp_name)</span><br><span class="hljs-keyword">from</span> timm.utils.summary <span class="hljs-keyword">import</span> get_outdir<br><br>output_dir = get_outdir(output_path, exp_name)  <span class="hljs-comment"># 返回的是创建好的文件夹的路径</span><br>checkpoint_dir = os.path.join(output_dir, <span class="hljs-string">&#x27;checkpoints&#x27;</span>)  <span class="hljs-comment"># 在checkpoints里面保存模型</span><br></code></pre></td></tr></table></figure><p>timm创建的模型是基于torch的，所以我们可以使用torch.save的方法保存模型。但是这里我们使用一个更加方便的方法去保存模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> timm.utils.checkpoint_saver <span class="hljs-keyword">import</span> CheckpointSaver<br><br>saver = CheckpointSaver(<br>    model=model,<br>    optimizer=optimizer,<br>    args=args,<br>    checkpoint_dir=checkpoint_dir,<br>    recovery_dir=checkpoint_dir,<br>    decreasing=<span class="hljs-literal">False</span>,  <span class="hljs-comment"># 以指标的升序排列模型</span><br>    max_history=<span class="hljs-number">3</span>  <span class="hljs-comment"># 只保存结果最好的3个模型</span><br>)<br></code></pre></td></tr></table></figure><p>使用上面的方法不仅可以保存模型，还可以自定义许多其他的功能，比我们自己手写模型的保存函数更加方便。当然如果不需要这些功能的话直接使用torch自带的保存功能也，可以参照如下格式简单保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">save_state = &#123;<br>    <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch,<br>    <span class="hljs-string">&#x27;state_dict&#x27;</span>: model.static_dict(),<br>    <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict()<br>&#125;<br>torch.save(save_state, save_path)<br></code></pre></td></tr></table></figure><h2 id="主要训练步骤">主要训练步骤</h2><p>完成上述的各个准备工作之后，可以正式编写模型的训练代码了。</p><p>我们用如下的结构编写训练的主要代码结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>    <span class="hljs-keyword">pass</span><br><span class="hljs-comment"># 这里放训练代码</span><br><span class="hljs-keyword">except</span> KetBoardInterrupt:<br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>按照这样的结构编写代码的好处是，运行的时候中断训练过程之后，代码能继续执行剩余的部分，可以留给一些文件的处理和信息保存。</p><p>把训练的代码替换try后面的pass，具体如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, args.epochs + <span class="hljs-number">1</span>):  <span class="hljs-comment"># 从1开始计算epoch，方便理解</span><br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用来计算acc</span><br>    model.train()  <span class="hljs-comment"># 将模型调整到train的模式，一些特殊算子如dropout会被启用</span><br>    <span class="hljs-keyword">for</span> batch_idx, (image, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        image = image.cuda()  <span class="hljs-comment"># 将数据转移到cuda设备上</span><br>        labels = labels.cuda()<br>        preds = model(image)  <span class="hljs-comment"># shape：（batchsize，num_classes）</span><br>        loss = loss_func(preds, labels)<br>        pred_labels = torch.argmax(preds.detach(), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># shape：batchsize</span><br>        correct += (pred_labels == labels).<span class="hljs-built_in">sum</span>()<br>        total += pred_labels.shape[<span class="hljs-number">0</span>]<br>        acc = <span class="hljs-number">100.0</span> * correct / total<br>        <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 按照log_interval设置间隔打印信息</span><br>            _logger.info(<br>                <span class="hljs-string">f&#x27;Train-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_train_iters&#125;</span>], &#x27;</span><br>                <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;loss.detach():<span class="hljs-number">.4</span>f&#125;</span>, Acc: <span class="hljs-subst">&#123;acc:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>            )<br>        optimizer.zero_grad()  <span class="hljs-comment"># 清理之前保存的梯度信息，不然会一直保存历史梯度</span><br>        loss.backward()  <span class="hljs-comment"># 计算反向传播</span><br>        optimizer.step()  <span class="hljs-comment"># 优化器对参数进行迭代更新</span><br><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 这一步很关键，不然验证集的梯度信息可能被记录</span><br>        model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 停用dropout等会对验证结果产生负面影响的算子</span><br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> batch_idx, (image, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_loader):<br>            image = image.cuda()<br>            labels = labels.cuda()<br>            preds = model(image)<br>            loss = loss_func(preds, labels)<br>            pred_labels = torch.argmax(preds, dim=<span class="hljs-number">1</span>)<br>            correct += (pred_labels == labels).<span class="hljs-built_in">sum</span>()<br>            total += pred_labels.shape[<span class="hljs-number">0</span>]<br>            acc = <span class="hljs-number">100.0</span> * correct / total<br>            <span class="hljs-keyword">if</span> batch_idx % args.log_interval == <span class="hljs-number">0</span>:<br>                _logger.info(<br>                    <span class="hljs-string">f&#x27;\tEval-<span class="hljs-subst">&#123;epoch&#125;</span>: [<span class="hljs-subst">&#123;batch_idx&#125;</span>/<span class="hljs-subst">&#123;total_eval_iters&#125;</span>], &#x27;</span><br>                    <span class="hljs-string">f&#x27;Loss: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.4</span>f&#125;</span>, Acc: <span class="hljs-subst">&#123;acc:<span class="hljs-number">.2</span>f&#125;</span>&#x27;</span><br>                )<br>        <span class="hljs-comment"># 按照指标保存模型，也可以按照loss的最小值保存模型，但在前面需要设置decreasing=True</span><br>        best_metric, best_epoch = saver.save_checkpoint(epoch, metric=acc)<br></code></pre></td></tr></table></figure><p>最后，我们可以加上一行代码输出最好指标的相关信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> best_metric <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    _logger.info(<span class="hljs-string">f&#x27;***** Best metric is <span class="hljs-subst">&#123;best_metric:<span class="hljs-number">.2</span>f&#125;</span>(<span class="hljs-subst">&#123;best_epoch&#125;</span>) *****.&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="参考结果">参考结果</h2><p>用resnet18在CIFAR10上跑5个epoch简单看看效果，证明能跑起来就算是完成了。</p>]]></content>
    
    
    <categories>
      
      <category>知识蒸馏</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
      <tag>timm</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
